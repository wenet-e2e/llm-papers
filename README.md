# llm-papers
List of Large Lanugage Model Papers


## GPTs by OpenAI

- GPT-1: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (2018)
- GPT-2: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019)
- GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf) (2020)
- InstructGPT: [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) (2022)
- ChatGPT: [Introducing ChatGPT](https://openai.com/blog/chatgpt), blog (2022)
- GPT-4: [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf) (2023)


## Prompt

- Chain-of-Thought: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf) (Google, NeurIPS, 2022)
- ReAct: [REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS](https://arxiv.org/pdf/2210.03629.pdf) (Google, ICLR, 2023)
- Self-Ask: [MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS](https://arxiv.org/pdf/2210.03350.pdf) (UW, 2023)

## Finetune
- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf) (Google, EMNLP, 2021)
- Prefix Tuning: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf) (Stanford, IJCNLP, 2021)
- LoRA: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf) (Microsoft, ICLR, 2022)
- P-Tuning: [P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks](https://aclanthology.org/2022.acl-short.8.pdf) (Tsinghua, ACL, 2022)
- P-Tuning v2: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf) (Tsinghua, ACL, 2022)
- AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512.pdf) (Georgia Tech, ICLR, 2023)
- QLoRA: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf) (UW, Submitted to NeurIPS, 2023)

## Multi Modality

### Image

### Speech

- AudioGPT: [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head](https://arxiv.org/pdf/2304.12995.pdf) (ZJU, 2023.04, [github](https://github.com/AIGC-Audio/AudioGPT))
- SpeechGPT: [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://arxiv.org/pdf/2305.11000.pdf) (FUDAN, 2023.05, [github](https://0nutation.github.io/SpeechGPT.github.io/))
